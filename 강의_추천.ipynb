{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBO7UfPLFeb_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from hanspell import spell_checker\n",
    "from eunjeon import Mecab\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import urllib.request\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaWeow-PFecD"
   },
   "source": [
    "## soynlp 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC-ZRGpDFecF",
    "outputId": "ee938457-69aa-4e63-bff3-0a9f43598a08"
   },
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename=\"2016-10-20.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRbqU5aEFecG",
    "outputId": "7defd4ec-2924-403e-9296-44aad69fd65a"
   },
   "outputs": [],
   "source": [
    "corpus = DoublespaceLineCorpus(\"2016-10-20.txt\")\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psL80WJuFecH"
   },
   "source": [
    "# 유사도 사용해서 추천시스템 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lxf9DgpeFecH",
    "outputId": "bb40aff7-7ec8-4d47-b09f-c68fe7ca5c40",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv(r'강의평.csv', encoding='cp949', index_col=0, low_memory=False).drop_duplicates([\"강의평\"])\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QbWCh8QFecI",
    "outputId": "6934f017-6dbc-45c3-eee7-7fac53f567ab"
   },
   "outputs": [],
   "source": [
    "# 맞춤법 교정 (오래걸려서 따로 저장했으니 그거 열어서 사용하시면 됩니다)\n",
    "\n",
    "# checked_sent = [spell_checker.check(sent).checked for sent in data['강의평'].tolist()]\n",
    "\n",
    "# checked_sent = []\n",
    "# for sent in data['강의평'].tolist():\n",
    "#     spelled = spell_checker.check(sent)\n",
    "#     res = spelled.checked\n",
    "#     checked_sent.append(res)\n",
    "\n",
    "# print(checked_sent[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj1JfOxRFecJ"
   },
   "outputs": [],
   "source": [
    "# data['교정된 강의평'] = checked_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4y9CmEapFecK"
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(r'강의평 수정.csv', encoding='cp949', index_col=0, low_memory=False).drop_duplicates([\"교정된 강의평\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syJH5rbZGnYc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWIbzmANFecM",
    "outputId": "c8eef793-fa3f-4292-b15f-761d9dee5a42"
   },
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ddx9ZkYeFecN",
    "outputId": "3815427a-16ee-4fca-f9e4-f3309d714e61"
   },
   "outputs": [],
   "source": [
    "data1[\"교정된 강의평\"].isnull().sum()\n",
    "data1 = data1.dropna(how='any')\n",
    "print(data1.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ig6yNy9AFecN",
    "outputId": "d4b5af43-8c4a-45a8-a341-d5c197b0b608"
   },
   "outputs": [],
   "source": [
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYOLsPsnFecO",
    "outputId": "1e5bba24-09f5-49f9-ac28-00a2aefea4f6"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data1[\"교정된 강의평\"].drop_duplicates())\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFf9wi_-FecO"
   },
   "outputs": [],
   "source": [
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzm-O3-wFecP",
    "outputId": "025fa0ce-7753-4189-84ad-4200e8a5e8db"
   },
   "outputs": [],
   "source": [
    "indices = pd.Series(data.index, index=data[\"교정된 강의평\"]).drop_duplicates()\n",
    "\n",
    "print(indices.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBufPkv6FecQ"
   },
   "outputs": [],
   "source": [
    "def get_recommendations(ques, cosine_sim=cosine_sim):\n",
    "    # 선택한 영화의 타이틀로부터 해당되는 인덱스를 받아옵니다. 이제 선택한 영화를 가지고 연산할 수 있습니다.\n",
    "    idx = indices[ques]\n",
    "\n",
    "    # 모든 영화에 대해서 해당 영화와의 유사도를 구합니다.\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # 유사도에 따라 영화들을 정렬합니다.\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 가장 유사한 10개의 영화를 받아옵니다.\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 인덱스를 받아옵니다.\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 제목을 리턴합니다.\n",
    "    return data['강의평'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXBmQ2jVFecQ",
    "outputId": "ecb86ab5-f8b0-40a4-d6f6-f73f3c86e7b8"
   },
   "outputs": [],
   "source": [
    "get_recommendations(\"좋긴 하지만 과제가 은근 귀찮 교수님은 좋아요 근데 교양 외국어 학점 잘 받을 생각은 하지 마세요...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Rwf1w8SFecQ"
   },
   "source": [
    "# Doc2Vec을 활용한 강의 추천!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-y8urPweFecQ"
   },
   "outputs": [],
   "source": [
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIhr9jNUFecQ",
    "outputId": "a41d8d12-3672-449b-85e7-c484f9d981a6"
   },
   "outputs": [],
   "source": [
    "lec_eval = data1[\"교정된 강의평\"].tolist()\n",
    "lec_nme = data1[\"강의 이름\"].tolist()\n",
    "lec_pf = data1[\"교수\"].tolist()\n",
    "\n",
    "print(len(lec_eval))\n",
    "print(lec_eval[:3])\n",
    "print(len(lec_nme))\n",
    "print(len(lec_pf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSU5BRoFFecR",
    "outputId": "4cd52a36-3c79-4be2-cdf9-4bb9a73a6ed7"
   },
   "outputs": [],
   "source": [
    "## 전처리\n",
    "def normalization(lec_ev):\n",
    "    pre_eval = []\n",
    "    for text in lec_ev:\n",
    "        temp = re.sub(r\"[^가-힣ㅏ-ㅣㄱ-ㅎ0-9A-Za-z\\~\\+\\-\\♥\\^\\!\\? ]\", \"\", text)\n",
    "        temp = re.sub(r\"\\<[a-z]+\\/\\>\", \"\", temp)\n",
    "        temp = emoticon_normalize(temp, num_repeats=2)\n",
    "        pre_eval.append(temp)\n",
    "\n",
    "    # 형태소 추출\n",
    "    morphs_eval = [mecab.morphs(x) for x in pre_eval]\n",
    "\n",
    "    return morphs_eval\n",
    "\n",
    "morphs_eval = normalization(lec_eval)\n",
    "print(morphs_eval[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zw9lbfDuFecR",
    "outputId": "dff93cc9-f5f2-48fc-ef04-089c5236554f"
   },
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "\n",
    "stop_words = [\"은\", \"는\", '이', '가', '아요', '입니다', '고', '으로', '다']\n",
    "\n",
    "def stop_(morphs_eval, ques=False):\n",
    "    merge_content = []\n",
    "    for morph in morphs_eval:\n",
    "        txt = ' '.join(morph)\n",
    "        merge_content.append(txt)\n",
    "\n",
    "    ### 불용어 리스트에 없는 단어들만 추가, 명사들로 이뤄진 리스트들\n",
    "    texts = [[word for word in document.split() if word not in stop_words]\n",
    "            for document in merge_content]\n",
    "    \n",
    "    if ques:\n",
    "        return texts[0]\n",
    "\n",
    "    return texts\n",
    "\n",
    "texts = stop_(morphs_eval)\n",
    "\n",
    "print(texts[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xft7IXtFecS",
    "outputId": "684d52dc-91d7-4ca5-b080-6d671b302016"
   },
   "outputs": [],
   "source": [
    "# 훈련하는데 오래걸리면 이 셀은 넘어가고\n",
    "\n",
    "#corpus 읽어서 태그달기(태그는 임의의 태그, 입력된 순서대로 달림)\n",
    "def read_corpus(texts):\n",
    "    for i, content in tqdm(enumerate(texts)):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(content, [i])\n",
    "\n",
    "corpus = list(read_corpus(texts))\n",
    "print(texts[:3])\n",
    "print(\"-\"*100)\n",
    "print(corpus[:3])\n",
    "#모델 학습, doc2vec\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=0, epochs=50, workers=4, dm=1, window=10) #모델 생성\n",
    "model.build_vocab(corpus) #모델 훈련 위한 vocab 생성\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs) #모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4p3GOO2FecS"
   },
   "outputs": [],
   "source": [
    "model.save(\"eta_model.doc2vec\")\n",
    "\n",
    "# 위 셀 넘어갔으면 아래 코드 주석 풀고 실행\n",
    "# model = Doc2Vec.load(\"eta_model.doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhsnZVEMFecS",
    "outputId": "f897165a-231b-40f2-a508-531b3195b0f1"
   },
   "outputs": [],
   "source": [
    "#main\n",
    "query = input('궁금하신 사항을 입력해 주세요: ')\n",
    "query = [query] #전처리에 인수 값으로 리스트 형식으로 넣어줘야 하기 때문에 문자열을 포함한 리스트 형식으로 변경\n",
    "\n",
    "##전처리 진행, 전처리한 문장을 학습한 doc2vec모델에 적용해 vectorize\n",
    "norm_content = normalization(query)\n",
    "preprocessed = stop_(norm_content, ques=True)\n",
    "print(preprocessed)\n",
    "vector = model.infer_vector(preprocessed) #학습된 모델로 벡터값 유추\n",
    "\n",
    "##가장 유사한 vector를 갖는 질문들 모아모아\n",
    "sims = model.docvecs.most_similar([vector], topn=len(model.docvecs))\n",
    "\n",
    "##유사도 상위 10개 tag들\n",
    "sims_tags = list(map(lambda x: x[0], sims[:10])) \n",
    "\n",
    "\n",
    "print('\\n\\n')\n",
    "for i, tag in enumerate(sims_tags):\n",
    "    print(i+1, ')', '유사도: ', sims[i][1])\n",
    "    print(\"유사한 강의평: %s\"%(lec_eval[tag]))\n",
    "    print('------질문------')\n",
    "    print(query)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('------강의명------')\n",
    "    print(i+1, \")\", lec_nme[tag])\n",
    "    print('\\n')\n",
    "    \n",
    "    print('------교수 이름------')\n",
    "    print(i+1,')', lec_pf[tag])\n",
    "    print('\\n\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "강의 추천.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
